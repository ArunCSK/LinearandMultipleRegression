
#############################################################################################################

##HDFDS Installation
1.Environment  Setup
sudo apt-get update 
#sudo apt-get install default-jdk
sudo apt install -y openjdk-8-jdk wget
sudo apt-get install rsync
sudo apt-get install ssh

2. Install ssh certificate
### ssh keygen for password-less connection
##Generate keys
##dsa and rsa key
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
##adding key to authorized key list
##dsa key
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
##rsa key
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


##key access permission
chmod og-wx ~/.ssh/authorized_keys

3.Download hadoop and install
wget http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz
sudo tar -xvf hadoop-3.2.0.tar.gz
sudo mv hadoop-3.2.0 /usr/local/hadoop

4. Adding Hadoop environment varaibale to bash file
#adding hadoop variables in bashrc files
##hadoop-env.sh path
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
#export HADOOP_MAPRED_HOME=$HADOOP_HOME/share/hadoop/mapreduce/
#export HADOOP_MAPRED_HOME=$HADOOP_HOME/share/hadoop/mapreduce/lib/
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMAN_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.libary.path=$HADOOP_HOME/lib"


source ~/.bashrc

#permission
sudo chown -R aruncsk:aruncsk /usr/local/hadoop

5.Add below configuration to mapred-site.xml
#sudo cp mapred-site.xml.template mapred-site.xml

#mapreduce config
vim etc/hadoop/mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
<property>
<name>yarn.app.mapreduce.am.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
<name>mapreduce.map.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
<name>mapreduce.reduce.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
</configuration>

6.Add below configuration to yarn-site.xml
#yarn config
vim etc/hadoop/yarn-site.xml
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>

7.Add below configuration to core-site.xml
#comman for hadoop config
vim etc/hadoop/core-site.xml
sudo vi core.site.xml
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>
</configuration>

8.Add below configuration to hdfs-site.xml
#hdfs config
vim etc/hadoop/hdfs-site.xml
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<porperty>
<name>dfs.name.dir</name>
<value>file:///home/aruncsk/hadoopspace/hdfs/namenode</value>
</porperty>
<porperty>
<name>dfs.data.dir</name>
<value>file:///home/aruncsk/hadoopspace/hdfs/namenode</value>
</porperty>
</configuration>

9.Add below configuration to hadoop-env.sh
#java path hadoop-env.sh
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
export JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/etc/opencv/lib

10. Make below directory from root
#make local dir for namenode and datanode
mkdir -p /home/aruncsk/hadoopspace/hdfs/namenode
mkdir -p /home/aruncsk/hadoopspace/hdfs/namenode/datanode

sudo chown -R aruncsk:aruncsk /usr/local/hadoop

#format namenode
hdfs namenode -format

#start services
start-dfs.sh
start-yarn.sh
start-all.sh
stop-all.sh

#hdfs commands
hdfs dfs -ls /
hdfs dfs -mkdir /arun

#put files
hdfs dfs -put Downloads/arun/text.txt /user/input

#read file blocks
hdfs fsck /user/input -files -blocks

#change block size
hdfs dfs -D dfs.blocksize=1024360 -put /user/input /arun

#read file blocks
hdfs fsck /user/input -files -blocks -locations

#############################################################################################################


Running mapreduce programs

#grand permission to files
chmod +x /home/hduser/reducer.py


hadoop jar contrib/streaming/hadoop-*streaming*.jar \
-file /home/aruncsk/mapper.py    -mapper /home/hduser/mapper.py \
-file /home/aruncsk/reducer.py   -reducer /home/hduser/reducer.py \
-input /arun/* -output /user/aruncsk/txt-output


hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming.jar -file /home/aruncsk/mapper.py    -mapper /home/aruncsk/mapper.py -file /home/aruncsk/reducer.py   -reducer /home/aruncsk/reducer.py -input /arun/* -output /user/aruncsk/txt-output



###############################################################################################################

Install Apache Spark Commands
1.Download Scala and extract
  wget https://downloads.lightbend.com/scala/2.13.0/scala-2.13.0.tgz
  sudo tar xvf scala-2.13.0.tgz

2.Set path in bash file
   sudo vim ~/.bashrc
   export SCALA_HOME=Downloads/scala-2.13.0/
   export PATH=$PATH:$SCALA_HOME/bin
   source ~/.bashrc

3.Download Spark and extract
  wget http://mirrors.estointernet.in/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
  tar xvf spark-2.4.3-bin-hadoop2.7.tgz	

4.Set path in bash file
  sudo vim ~/.bashrc
  export SPARK_HOME=/home/aruncsk/spark-2.4.3-bin-hadoop2.7
  export PATH=$SPARK_HOME/bin:$PATH
  source ~/.bashrc

5. Start service standalone
  cd $SPARK_HOME
  ./sbin/start-master.sh

6. Enter spark shell
   spark-shell 
 
7 .Read text file
  create a text file in $SPARK_HOME directory
  val data = sc.textFile("data.txt")
  var hFile = sc.textFile("hdfs://localhost:9000/inp")

  data.count()
  data.collect()
  
  MapReduce using functions
  val wc = hFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
  val wc = data.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
  
  wc.collect()
  wc.take(10)


#########################################################################################################################

Installting Kafka 

1.Dowload and extract files
 wget http://www-us.apache.org/dist/kafka/2.2.1/kafka_2.12-2.2.1.tgz
 tar xzf kafka_2.12-2.2.1.tgz
 mv kafka_2.12-2.2.1 /usr/local/kafka

2. Start kafka server
   cd /home/aruncsk/kafka_2.12-2.2.1/
   bin/zookeeper-server-start.sh config/zookeeper.properties
   bin/kafka-server-start.sh config/server.properties

##test zookeeper running from telnet 
telnet localhost 2181

3. Create topic 
  bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testTopic
  
4. check topic 
  bin/kafka-topics.sh --list --zookeeper localhost:2181
  #describe testTopic
  bin/kafka-topics.sh --describe --topic testTopic --zookeeper localhost:2181

5. Producer
  bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testTopic

6. Consumer
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic testTopic --from-beginning


##########################################################################################################################

Week5 Stock model predections

Aanconda isntallation

cd /tmp
curl -O https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh
sha256sum Anaconda3-2019.03-Linux-x86_64.sh
bash Anaconda3-2019.03-Linux-x86_64.sh
## anconda prefix = PREFIX=/home/aruncsk/anaconda3
source ~/.bashrc

python3 -m pip install --upgrade pip
python3 -m pip install jupyter

##jupyter notebook
http://13.70.24.114:8888/


#install chrome
#wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
#sudo dpkg -i google-chrome-stable_current_amd64.deb
#ssh -L 8888:localhost:8888 admin1@192.168.0.255

##notebook configuration
https://jupyter-notebook.readthedocs.io/en/stable/config_overview.html
 jupyter notebook --generate-config
 vi ./.jupyter/jupyter_notebook_config.py
 c.NotebookApp.ip = '0.0.0.0'


https://mapr.com/blog/configure-jupyter-spark-python/



		

  





