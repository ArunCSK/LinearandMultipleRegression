
#############################################################################################################

##HDFDS Installation
1.Environment  Setup
sudo apt-get update 
#sudo apt-get install default-jdk
sudo apt install -y openjdk-8-jdk wget
sudo apt-get install rsync
sudo apt-get install ssh

2. Install ssh certificate
### ssh keygen for password-less connection
##Generate keys
##dsa and rsa key
ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
##adding key to authorized key list
##dsa key
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
##rsa key
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


##key access permission
chmod og-wx ~/.ssh/authorized_keys

3.Download hadoop and install
wget http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz
sudo tar -xvf hadoop-3.2.0.tar.gz
sudo mv hadoop-3.2.0 /usr/local/hadoop

4. Adding Hadoop environment varaibale to bash file
#adding hadoop variables in bashrc files
##hadoop-env.sh path
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
#export HADOOP_MAPRED_HOME=$HADOOP_HOME/share/hadoop/mapreduce/
#export HADOOP_MAPRED_HOME=$HADOOP_HOME/share/hadoop/mapreduce/lib/
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMAN_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.libary.path=$HADOOP_HOME/lib"


source ~/.bashrc

#permission
sudo chown -R aruncsk:aruncsk /usr/local/hadoop

5.Add below configuration to mapred-site.xml
#sudo cp mapred-site.xml.template mapred-site.xml

#mapreduce config
vim etc/hadoop/mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
<property>
<name>yarn.app.mapreduce.am.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
<name>mapreduce.map.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
<name>mapreduce.reduce.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
</configuration>

6.Add below configuration to yarn-site.xml
#yarn config
vim etc/hadoop/yarn-site.xml
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>

7.Add below configuration to core-site.xml
#comman for hadoop config
vim etc/hadoop/core-site.xml
sudo vi core.site.xml
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>
</configuration>

8.Add below configuration to hdfs-site.xml
#hdfs config
vim etc/hadoop/hdfs-site.xml
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<porperty>
<name>dfs.name.dir</name>
<value>file:///home/aruncsk/hadoopspace/hdfs/namenode</value>
</porperty>
<porperty>
<name>dfs.data.dir</name>
<value>file:///home/aruncsk/hadoopspace/hdfs/namenode</value>
</porperty>
</configuration>

9.Add below configuration to hadoop-env.sh
#java path hadoop-env.sh
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
export JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/etc/opencv/lib

10. Make below directory from root
#make local dir for namenode and datanode
mkdir -p /home/aruncsk/hadoopspace/hdfs/namenode
mkdir -p /home/aruncsk/hadoopspace/hdfs/namenode/datanode

sudo chown -R aruncsk:aruncsk /usr/local/hadoop

#format namenode
hdfs namenode -format

#start services
start-dfs.sh
start-yarn.sh
start-all.sh
stop-all.sh

#hdfs commands
hdfs dfs -ls /
hdfs dfs -mkdir /arun

#############################################################################################################


Running mapreduce programs

#grand permission to files
chmod +x /home/hduser/reducer.py


hadoop jar contrib/streaming/hadoop-*streaming*.jar \
-file /home/aruncsk/mapper.py    -mapper /home/hduser/mapper.py \
-file /home/aruncsk/reducer.py   -reducer /home/hduser/reducer.py \
-input /arun/* -output /user/aruncsk/txt-output


hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming.jar -file /home/aruncsk/mapper.py    -mapper /home/hduser/mapper.py -file /home/aruncsk/reducer.py   -reducer /home/hduser/reducer.py -input /arun/* -output /user/aruncsk/txt-output


